{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building machine learning models, one of the primary challenges is to ensure that the model generalizes well to new, unseen data. We can measure how well the model generalises using independent Test set. Some of the evaluation metrics that can be used to measure performance on test set are : \n",
    "- prediction accuracy\n",
    "- mis-classification error\n",
    "\n",
    "Simply we say that a good model has - \n",
    "- high generalization accuracy\n",
    "- low generalization error\n",
    "\n",
    "**Now, overfitting and underfitting are two terms that we can use to diagnose a machine learning model based on the training and test set performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting occurs when a model learns the training data too well, capturing noise and details that do not generalize to unseen data. This usually happens when the model is too complex, such as having too many parameters relative to the number of observations. \n",
    "\n",
    "If we say in very simple and concise language : Overfitting occurs when model starts fitting the noise. It thinks noise also to be an important structure of data that needs to get modelled. This happens because the model is too complex(more parameters than required) for data.\n",
    "\n",
    "### Indicators of Overfitting\n",
    "\n",
    "- **Low Training Error, High Test Error**\n",
    "- **High Training Accuracy, Low Test Accuracy**\n",
    "- **The model performs exceptionally well on training data but poorly on validation/test data**\n",
    "- **Complex Model**: The model may have large number of parameters. For eg: Higher order polynomial fitting simple data.\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "<img src=\"https://keeeto.github.io/assets/images/Overfitting.png\" alt=\"Overfitting Example\" width=\"430px\">\n",
    "\n",
    "[Image Source : keeto.github.io](https://keeeto.github.io/blog/bias_variance/)\n",
    "\n",
    "\n",
    "### How to Address Overfitting\n",
    "\n",
    "1. **Simplify the Model**: Reduce the complexity of the model by decreasing the number of features or parameters.\n",
    "2. **Regularization**: Techniques like L1 or L2 regularization can penalize large coefficients, helping to prevent overfitting.\n",
    "3. **More Data**: Increasing the size of the training dataset can help the model to generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting\n",
    "\n",
    "### What is Underfitting?\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture the underlying structure of the data. This usually happens when the model has too few parameters, making it unable to learn the patterns in the data.\n",
    "\n",
    "If we say in very simple and concise language : Underfitting occurs when model starts almost everything as noise. It does not fit the actual structure for given data leave about noise. This happens when the model has less number of parameters so that it is not powerful enough to model given data's structure. \n",
    "\n",
    "### Indicators of Underfitting\n",
    "\n",
    "- **High Training Error, High Validation/Test Error**\n",
    "- **low Training Accuracy, Low Test Accuracy**\n",
    "- **The model performs poorly on both training and validation/test data.**\n",
    "- **Simple Model**: The model may have too few features or parameters, making it incapable of capturing complexities in the data.\n",
    "\n",
    "### Visual Example\n",
    "<img src=\"https://keeeto.github.io/assets/images/Underfitting.png\" alt=\"Overfitting Example\" width=\"430px\">\n",
    "\n",
    "[Image Source : keeto.github.io](https://keeeto.github.io/blog/bias_variance/)\n",
    "\n",
    "### How to Address Underfitting\n",
    "\n",
    "1. **Increase Model Complexity**: Add more features or parameters to the model.\n",
    "2. **Feature Engineering**: Create new features that can help capture the underlying patterns in the data.\n",
    "3. **Reduce Bias**: Use techniques to reduce the bias in the model.\n",
    "\n",
    "## Finding the Right Balance\n",
    "\n",
    "The goal is to find a balance between overfitting and underfitting, which is often referred to as the bias-variance tradeoff. \n",
    "**Bias** refers to errors due to overly simplistic models, while **variance** refers to errors due to overly complex models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy and pandas as usual\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-Learn for fitting models\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# For plotting in the notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Default parameters for plots\n",
    "matplotlib.rcParams['font.size'] = 12\n",
    "matplotlib.rcParams['figure.titlesize'] = 16\n",
    "matplotlib.rcParams['figure.figsize'] = [8, 6]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation for Polynomial Regression Example\n",
    "we generate the dataset that will be used to demonstrate the concepts of overfitting and underfitting in polynomial regression. We set the random seed to 42 using `np.random.seed(42)` to ensure reproducible results. This allows us to generate the same random numbers each time the code is run.\n",
    "\n",
    "**Underlying distribution or Generating Function**:\n",
    "   - In our case, the underlying function from which data comes is defined as a sine function: \\( y = \\sin(2\\pi x) \\).\n",
    "   - We generate 120 `x` values uniformly distributed between 0 and 1, sorted in ascending order.\n",
    "   - The corresponding `y` values are calculated using the true generating function and then a small amount of Gaussian noise is added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# \"True\" generating function representing a process in real life\n",
    "# def true_gen(x):\n",
    "#     y = np.sin(1.2 * x * np.pi) \n",
    "#     return(y)\n",
    "\n",
    "def true_gen(x):\n",
    "    y = np.sin(2 * np.pi * x) \n",
    "    return y\n",
    "\n",
    "x = np.sort(np.random.rand(120))\n",
    "y = true_gen(x) + 0.1 * np.random.randn(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random indices for creating training and testing sets\n",
    "random_ind = np.random.choice(list(range(120)), size = 120, replace=False)\n",
    "xt = x[random_ind]\n",
    "yt = y[random_ind]\n",
    "\n",
    "# Training and testing observations\n",
    "train = xt[:int(0.7 * len(x))]\n",
    "test = xt[int(0.7 * len(x)):]\n",
    "\n",
    "y_train = yt[:int(0.7 * len(y))]\n",
    "y_test = yt[int(0.7 * len(y)):]\n",
    "\n",
    "# Model the true curve\n",
    "x_linspace = np.linspace(0, 1, 1000)\n",
    "y_true = true_gen(x_linspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize observations and true curve\n",
    "plt.plot(train, y_train, 'ko', label = 'Train'); \n",
    "plt.plot(test, y_test, 'ro', label = 'Test')\n",
    "plt.plot(x_linspace, y_true, 'b-', linewidth = 2, label = 'True Function')\n",
    "plt.legend()\n",
    "plt.xlabel('x'); plt.ylabel('y'); plt.title('Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Polynomial Regression Model Function\n",
    "\n",
    "The `fit_poly` function fits a polynomial regression model to the training data and evaluates its performance on both the training and testing datasets. The function also provides options for plotting the results and returning key metrics.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `train`: Array-like, shape `(n_samples,)` :  Training data features.\n",
    "  \n",
    "- `y_train`: Array-like, shape `(n_samples,)` : Training data target values.\n",
    "  \n",
    "- `test`: Array-like, shape `(n_samples,)` : Testing data features.\n",
    "  \n",
    "- `y_test`: Array-like, shape `(n_samples,)` : Testing data target values.\n",
    "  \n",
    "- `degrees`: int : The degree of the polynomial to fit.\n",
    "  \n",
    "- `plot`: str, optional, default='train' : If 'train', plots the model fitted on training data; if 'test', plots the model predictions on the test data.\n",
    "  \n",
    "- `return_scores`: bool, optional, default=False : If True, returns the training error, testing error, cross-validation score, and model coefficients.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `training_error`: float : Mean squared error on the training data.\n",
    "  \n",
    "- `testing_error`: float : Mean squared error on the testing data.\n",
    "  \n",
    "- `model.coef_`: array :  Coefficients of the polynomial regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_poly(train, y_train, test, y_test, degrees, plot='train', return_scores=False):\n",
    "    # Create a polynomial transformation of features\n",
    "    features = PolynomialFeatures(degree=degrees, include_bias=False)\n",
    "    \n",
    "    # Reshape training features for use in scikit-learn and transform features\n",
    "    train = train.reshape((-1, 1))\n",
    "    train_trans = features.fit_transform(train)\n",
    "    \n",
    "    # Create the linear regression model and train\n",
    "    model = LinearRegression()\n",
    "    model.fit(train_trans, y_train)\n",
    "    \n",
    "    # Training predictions and error\n",
    "    train_predictions = model.predict(train_trans)\n",
    "    training_error = mean_squared_error(y_train, train_predictions)\n",
    "    \n",
    "    # Format test features\n",
    "    test = test.reshape((-1, 1))\n",
    "    test_trans = features.fit_transform(test)\n",
    "    \n",
    "    # Test set predictions and error\n",
    "    test_predictions = model.predict(test_trans)\n",
    "    testing_error = mean_squared_error(y_test, test_predictions)\n",
    "    \n",
    "    # Find the model curve and the true curve\n",
    "    x_curve = np.linspace(0, 1, 100)\n",
    "    x_curve = x_curve.reshape((-1, 1))\n",
    "    x_curve_trans = features.fit_transform(x_curve)\n",
    "    \n",
    "    # Model curve\n",
    "    model_curve = model.predict(x_curve_trans)\n",
    "    \n",
    "    # True curve\n",
    "    y_true_curve = true_gen(x_curve[:, 0])\n",
    "    \n",
    "    # Plot observations, true function, and model predicted function\n",
    "    if plot == 'train':\n",
    "        plt.plot(train[:, 0], y_train, 'ko', label = 'Observations')\n",
    "        plt.plot(x_curve[:, 0], y_true_curve, linewidth = 4, label = 'True Function')\n",
    "        plt.plot(x_curve[:, 0], model_curve, linewidth = 4, label = 'Model Function')\n",
    "        plt.xlabel('x'); plt.ylabel('y')\n",
    "        plt.legend()\n",
    "        plt.ylim(-1, 1.5); plt.xlim(0, 1)\n",
    "        plt.title('{} Degree Model on Training Data'.format(degrees))\n",
    "        plt.show()\n",
    "    elif plot == 'test':\n",
    "        # Plot the test observations and test predictions\n",
    "        plt.plot(test, y_test, 'o', label = 'Test Observations')\n",
    "        plt.plot(x_curve[:, 0], y_true_curve, 'b-', linewidth = 2, label = 'True Function')\n",
    "        plt.plot(test, test_predictions, 'ro', label = 'Test Predictions')\n",
    "        plt.ylim(-1, 1.5); plt.xlim(0, 1)\n",
    "        plt.legend(), plt.xlabel('x'), plt.ylabel('y'); plt.title('{} Degree Model on Testing Data'.format(degrees)), plt.show();\n",
    "    \n",
    "    # Return the metrics and coefficients\n",
    "    if return_scores:\n",
    "        return training_error, testing_error, model.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_poly(train, y_train, test, y_test, degrees = 1, plot='train')\n",
    "\n",
    "fit_poly(train, y_train, test, y_test, degrees = 1, plot='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_poly(train, y_train, test, y_test, plot='train', degrees = 25)\n",
    "fit_poly(train, y_train, test, y_test, degrees=25, plot='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# degrees = [1, 2, 3, 7, 10, 15, 20, 25, 30, 35]\n",
    "degrees = [int(x) for x in np.linspace(1, 40, 40)]\n",
    "results = pd.DataFrame(columns=['train_error', 'test_error'], index=degrees)\n",
    "coefficients = []\n",
    "\n",
    "# Try each value of degrees for the model and record results\n",
    "for degree in degrees:\n",
    "    degree_results = fit_poly(train, y_train, test, y_test, degree, plot=False, return_scores=True)\n",
    "    results.loc[degree, 'train_error'] = degree_results[0]\n",
    "    results.loc[degree, 'test_error'] = degree_results[1]\n",
    "\n",
    "    # Store coefficients in a dictionary with the degree\n",
    "    coefs = degree_results[2]\n",
    "    coef_dict = {'degree': degree}\n",
    "    coef_dict.update({f'coef_{i}': coef for i, coef in enumerate(coefs)})\n",
    "    coefficients.append(coef_dict)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "coefficients_df = pd.DataFrame(coefficients)\n",
    "coefficients_df.fillna(value=0, inplace=True)\n",
    "coefficients_df.set_index('degree', inplace=True)\n",
    "coefficients_df = coefficients_df.T\n",
    "coefficients_df = coefficients_df.applymap(lambda x: f\"{x:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "desired_degrees = [1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 40]\n",
    "coefficients_df[desired_degrees]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_poly(train, y_train, test, y_test, degrees=4, plot='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_poly(train, y_train, test, y_test, degrees=4, plot='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('10 Lowest Training Errors\\n')\n",
    "train_eval = results.sort_values('train_error').reset_index(level=0).rename(columns={'index': 'degrees'})\n",
    "train_eval.loc[:,['degrees', 'train_error']] .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('10 Lowest Testing Errors\\n')\n",
    "train_eval = results.sort_values('test_error').reset_index(level=0).rename(columns={'index': 'degrees'})\n",
    "train_eval.loc[:,['degrees', 'test_error']] .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(results.index, results['train_error'], 'b-o', ms=6, label = 'Training Error')\n",
    "plt.plot(results.index, results['test_error'], 'r-*', ms=6, label = 'Testing Error')\n",
    "min_test_error_deg = results['test_error'].idxmin()\n",
    "plt.axvline(min_test_error_deg, color='green', linestyle='--', label=f'Min Test Error Degree: {min_test_error_deg}')\n",
    "plt.legend(loc=2); plt.xlabel('Degrees (Model Complexity)'); plt.ylabel('Error'); plt.title('Training and Testing Curves');\n",
    "plt.ylim(0, 0.10); plt.show()\n",
    "\n",
    "print('\\nMinimum Training Error occurs at {} degrees.'.format(int(np.argmin(results['train_error']))))\n",
    "print('Minimum Testing Error occurs at {} degrees.\\n'.format(results['test_error'].idxmin()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset following y = x^2\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = X**2 + np.random.normal(scale=1.0, size=X.shape)  # Adding some noise\n",
    "\n",
    "# Define the number of samples to draw from the dataset\n",
    "num_samples = 3\n",
    "sample_size = 20\n",
    "\n",
    "# Colors for different samples\n",
    "colors = sns.color_palette(\"husl\", num_samples)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Train a polynomial regression model (degree 8) on different samples and plot the results\n",
    "for i in range(num_samples):\n",
    "    # Randomly select a sample from the dataset\n",
    "    indices = np.random.choice(range(len(X)), size=sample_size, replace=False)\n",
    "    X_sample = X[indices]\n",
    "    y_sample = y[indices]\n",
    "    \n",
    "    # Create a polynomial regression model of degree 8\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_sample, y_sample)\n",
    "    \n",
    "    # Predict y values across the entire range for plotting the fitted curve\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Plot the sample data points\n",
    "    plt.scatter(X_sample, y_sample, color=colors[i], s=80, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    # Plot the fitted polynomial curve\n",
    "    plt.plot(X, y_pred, color=colors[i], linestyle='-', linewidth=2.5, label=f'D_train {i+1} Fit')\n",
    "\n",
    "# Plot the original function for reference\n",
    "plt.plot(X, X**2, color='black', linestyle='--', linewidth=2.5, label=r'$y = x^2$ (True)')\n",
    "\n",
    "# Enhance the plot with labels, title, and legend\n",
    "plt.xlabel(r'$X$', fontsize=16)\n",
    "plt.ylabel(r'$y$', fontsize=16)\n",
    "plt.title(r'Polynomial Regression Fits on Different Samples ($Degree = 8$)', fontsize=20)\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset following y = x^2\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = X**2 + np.random.normal(scale=1.0, size=X.shape)  # Adding some noise\n",
    "\n",
    "# Define the number of samples to draw from the dataset\n",
    "num_samples = 5\n",
    "sample_size = 20\n",
    "\n",
    "# Colors for different samples\n",
    "colors = sns.color_palette(\"husl\", num_samples)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Train a polynomial regression model (degree 8) on different samples and plot the results\n",
    "for i in range(num_samples):\n",
    "    # Randomly select a sample from the dataset\n",
    "    indices = np.random.choice(range(len(X)), size=sample_size, replace=False)\n",
    "    X_sample = X[indices]\n",
    "    y_sample = y[indices]\n",
    "    \n",
    "    # Create a polynomial regression model of degree 8\n",
    "    model = make_pipeline(PolynomialFeatures(degree=8), LinearRegression())\n",
    "    model.fit(X_sample, y_sample)\n",
    "    \n",
    "    # Predict y values across the entire range for plotting the fitted curve\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Plot the sample data points\n",
    "    plt.scatter(X_sample, y_sample, color=colors[i], s=50, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    # Plot the fitted polynomial curve\n",
    "    plt.plot(X, y_pred, color=colors[i], linestyle='-', linewidth=2.5, label=f'D_train {i+1} Fit')\n",
    "\n",
    "\n",
    "# Plot the original function for reference\n",
    "plt.plot(X, X**2, color='black', linestyle='--', linewidth=2.5, label=r'$y = x^2$ (True)')\n",
    "\n",
    "\n",
    "# Plot a vertical line at the random test point\n",
    "test_point = -2.5\n",
    "plt.axvline(x=test_point, color='red', linestyle='--', linewidth=2, label=f'Random Test Point at x={test_point:.2f}')\n",
    "\n",
    "# Enhance the plot with labels, title, and legend\n",
    "plt.xlabel(r'$X$', fontsize=16)\n",
    "plt.ylabel(r'$y$', fontsize=16)\n",
    "plt.title(r'Polynomial Regression Fits on Different Samples ($Degree = 8$)', fontsize=20)\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bias_variance(train, y_train, test, y_test, degrees, n_iterations=100, random_seed=None):\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)  # Set the random seed\n",
    "    \n",
    "    features = PolynomialFeatures(degree=degrees, include_bias=False)\n",
    "    train = train.reshape((-1, 1))\n",
    "    test = test.reshape((-1, 1))\n",
    "    \n",
    "    # Transform train and test data once, outside the loop\n",
    "    train_transformed = features.fit_transform(train)\n",
    "    test_transformed = features.fit_transform(test)\n",
    "    \n",
    "    all_predictions_train = []\n",
    "    all_predictions_test = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Resample the training data\n",
    "        indices = np.random.choice(range(len(train)), len(train), replace=True)\n",
    "        train_resample = train[indices]\n",
    "        y_train_resample = y_train[indices]\n",
    "        \n",
    "        # Transform the resampled train data  X--- phi(X)\n",
    "        train_resample_trans = features.fit_transform(train_resample)\n",
    "        \n",
    "        # Fit the model on the resampled training data\n",
    "        model = LinearRegression().fit(train_resample_trans, y_train_resample)\n",
    "        \n",
    "        # Predict on the original train and test sets\n",
    "        all_predictions_train.append(model.predict(train_transformed))\n",
    "        all_predictions_test.append(model.predict(test_transformed))\n",
    "    \n",
    "    # Convert lists to arrays\n",
    "    all_predictions_train = np.array(all_predictions_train)\n",
    "    all_predictions_test = np.array(all_predictions_test)\n",
    "    \n",
    "    # Bias Calculation\n",
    "    bias_train = np.mean((np.mean(all_predictions_train, axis=0) - y_train) ** 2)\n",
    "    bias_test = np.mean((np.mean(all_predictions_test, axis=0) - y_test) ** 2)\n",
    "    \n",
    "    # Variance Calculation\n",
    "    variance_train = np.mean(np.var(all_predictions_train, axis=0))\n",
    "    variance_test = np.mean(np.var(all_predictions_test, axis=0))\n",
    "    \n",
    "    # MSE Calculation\n",
    "    mse_train = bias_train + variance_train\n",
    "    mse_test = bias_test + variance_test\n",
    "    \n",
    "    return {\n",
    "        'bias_train': bias_train,\n",
    "        'variance_train': variance_train,\n",
    "        'mse_train': mse_train,\n",
    "        'bias_test': bias_test,\n",
    "        'variance_test': variance_test,\n",
    "        'mse_test': mse_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_variance(train, y_train, test, y_test, max_degree=15, n_iterations=100, random_seed=None):\n",
    "    biases_train, variances_train, biases_test, variances_test = [], [], [], []\n",
    "    degrees = range(1, max_degree + 1)\n",
    "    \n",
    "    for degree in degrees:\n",
    "        results = calculate_bias_variance(train, y_train, test, y_test, degree, n_iterations=n_iterations, random_seed=random_seed)\n",
    "        biases_train.append(results['bias_train'])\n",
    "        variances_train.append(results['variance_train'])\n",
    "        biases_test.append(results['bias_test'])\n",
    "        variances_test.append(results['variance_test'])\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Custom color palette\n",
    "    color_bias = '#FF6F61'\n",
    "    color_variance = '#6B5B95'\n",
    "    \n",
    "    # Plot for Training Data\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(degrees, biases_train, label='Bias^2 (Train)', color=color_bias, marker='o', linestyle='-', linewidth=2, markersize=6)\n",
    "    plt.plot(degrees, variances_train, label='Variance (Train)', color=color_variance, marker='s', linestyle='--', linewidth=2, markersize=6)\n",
    "    plt.xlabel('Model Complexity (Polynomial Degree)', fontsize=14)\n",
    "    plt.ylabel('Error', fontsize=14)\n",
    "    plt.title('Bias-Variance Tradeoff on Training Data', fontsize=16, weight='bold')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Plot for Testing Data\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(degrees, biases_test, label='Bias^2 (Test)', color=color_bias, marker='o', linestyle='-', linewidth=2, markersize=6)\n",
    "    plt.plot(degrees, variances_test, label='Variance (Test)', color=color_variance, marker='s', linestyle='--', linewidth=2, markersize=6)\n",
    "    plt.xlabel('Model Complexity (Polynomial Degree)', fontsize=14)\n",
    "    plt.ylabel('Error', fontsize=14)\n",
    "    plt.title('Bias-Variance Tradeoff on Testing Data', fontsize=16, weight='bold')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_bias_variance(train, y_train, test, y_test, max_degree=17, n_iterations=100, random_seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
