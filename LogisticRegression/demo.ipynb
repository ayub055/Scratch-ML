{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "# %matplotlib\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    prob = 1 / ( 1 + np.exp(-x))\n",
    "    \n",
    "    if derivative:\n",
    "        d_sigmoid = prob * (1 - prob)\n",
    "        return d_sigmoid\n",
    "    return prob\n",
    "\n",
    "def h_theta(x, w, b):\n",
    "    z = (x @ w) + b\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef111d7880fd41f5b868e7f8cae0e953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='b', max=5.0, min=-5.0, step=0.5), FloatSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper.utils import plot_h_theta_1d\n",
    "interactive_plot = interact(plot_h_theta_1d,\n",
    "         b=widgets.FloatSlider(min=-5, max=5, step=0.5, value=0),\n",
    "         w=widgets.FloatSlider(min=-5, max=5, step=1, value=1) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62d1dae67c342fc8ba1e04ec71b6f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='b', max=5.0, min=-5.0, step=1.0), FloatSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper.utils import plot_linear_hyperplane\n",
    "interact(plot_linear_hyperplane,\n",
    "         b=widgets.FloatSlider(min=-5, max=5, step=1, value=0),\n",
    "         w_magnitude=widgets.FloatSlider(min=0, max=3, step=0.2, value=2),\n",
    "         w_angle=widgets.FloatSlider(min=-np.pi, max=np.pi, step=np.pi/8, value=0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef701517fa264141b7142fbffa59ce73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='b', max=5.0, min=-5.0, step=1.0), FloatSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper.utils import plot_linear_logistic_2d\n",
    "interact(plot_linear_logistic_2d,\n",
    "         b=widgets.FloatSlider(min=-5, max=5, step=1, value=0),\n",
    "         w_magnitude=widgets.FloatSlider(min=0.5, max=5, step=0.5, value=2),\n",
    "         w_angle=widgets.FloatSlider(min=-np.pi, max=np.pi, step=0.5, value=0) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "print(X.shape)\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling (standardize the data)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (696426,2) and (30,) not aligned: 2 (dim 1) != 30 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Example usage with training\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Assuming you have your training data ready: X_train, y_train\u001b[39;00m\n\u001b[1;32m    114\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 90\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Capture and save the decision boundary every 100 iterations\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;66;03m# print(f\"Loss at iteration {i}: {epoch_loss:.4f}\")\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_decision_boundary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m, in \u001b[0;36mLogisticRegression.plot_decision_boundary\u001b[0;34m(self, X, y, iteration)\u001b[0m\n\u001b[1;32m     39\u001b[0m xx, yy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(np\u001b[38;5;241m.\u001b[39marange(x_min, x_max, \u001b[38;5;241m0.01\u001b[39m),\n\u001b[1;32m     40\u001b[0m                      np\u001b[38;5;241m.\u001b[39marange(y_min, y_max, \u001b[38;5;241m0.01\u001b[39m))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Predict probabilities for each point in the meshgrid\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m Z \u001b[38;5;241m=\u001b[39m Z\u001b[38;5;241m.\u001b[39mreshape(xx\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Plot contour and training examples\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 100\u001b[0m, in \u001b[0;36mLogisticRegression.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Predict binary labels\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[7], line 96\u001b[0m, in \u001b[0;36mLogisticRegression.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Predict probabilities\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_h_theta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mLogisticRegression._h_theta\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_h_theta\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Hypothesis function: sigmoid(X * weights)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sigmoid(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (696426,2) and (30,) not aligned: 2 (dim 1) != 30 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, batch_size=32):\n",
    "        # Initialize parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None  # To be initialized in fit()\n",
    "        self.losses = []  # To keep track of loss over iterations\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        # Sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _h_theta(self, X):\n",
    "        # Hypothesis function: sigmoid(X * weights)\n",
    "        return self._sigmoid(np.dot(X, self.weights))\n",
    "    \n",
    "    def _cross_entropy_loss(self, h, y):\n",
    "        # Cross-entropy loss function\n",
    "        return - np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    \n",
    "    def _compute_loss_and_gradient(self, X, y):\n",
    "        # Compute loss and gradient for current weights\n",
    "        m = X.shape[0]\n",
    "        h = self._h_theta(X)\n",
    "        loss = self._cross_entropy_loss(h, y)\n",
    "        grad = np.dot(X.T, (h - y)) / m  # Gradient of loss w.r.t. weights\n",
    "        return loss, grad\n",
    "\n",
    "    def plot_decision_boundary(self, X, y, iteration):\n",
    "        # Create a meshgrid to cover the feature space\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                             np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "        # Predict probabilities for each point in the meshgrid\n",
    "        Z = self.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # Plot contour and training examples\n",
    "        plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=50, cmap=plt.cm.coolwarm)\n",
    "        plt.title(f'Logistic Regression Decision Boundary - Iteration {iteration}')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "\n",
    "        # Save the plot\n",
    "        plt.savefig(f'frames/decision_boundary_{iteration}.png')\n",
    "        plt.close()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)  # Initialize weights to zeros\n",
    "\n",
    "        # Create directory to store frames if it doesn't exist\n",
    "        if not os.path.exists('frames'):\n",
    "            os.makedirs('frames')\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            # Shuffle the data correctly\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X[indices]  # Shuffle rows of X\n",
    "            y_shuffled = y[indices]  # Shuffle rows of y (same order)\n",
    "\n",
    "            # Mini-batch gradient descent\n",
    "            for start in range(0, m, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_X = X_shuffled[start:end]  # Slice the shuffled batch of X\n",
    "                batch_y = y_shuffled[start:end]  # Slice the shuffled batch of y\n",
    "                \n",
    "                if len(batch_X) == 0:  # Safety check for empty batch\n",
    "                    continue\n",
    "\n",
    "                loss, grad = self._compute_loss_and_gradient(batch_X, batch_y)\n",
    "                self.weights -= self.learning_rate * grad  # Update weights\n",
    "            \n",
    "            # Calculate loss on the full dataset after each epoch\n",
    "            epoch_loss, _ = self._compute_loss_and_gradient(X, y)\n",
    "            self.losses.append(epoch_loss)\n",
    "\n",
    "            # Capture and save the decision boundary every 100 iterations\n",
    "            if i % 10 == 0:\n",
    "                # print(f\"Loss at iteration {i}: {epoch_loss:.4f}\")\n",
    "                self.plot_decision_boundary(X, y, i)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Predict probabilities\n",
    "        return self._h_theta(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predict binary labels\n",
    "        return np.round(self.predict_proba(X))\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        # Plot the loss over iterations\n",
    "        plt.plot(range(len(self.losses)), self.losses, marker='o')\n",
    "        plt.title(\"Loss over iterations\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage with training\n",
    "# Assuming you have your training data ready: X_train, y_train\n",
    "model = LogisticRegression(learning_rate=0.02, max_iter=1000, batch_size=8)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "# Create a GIF from the saved frames\n",
    "images = []\n",
    "for i in range(0, 1000, 10):  # Adjust step based on how frequently you saved frames\n",
    "    filename = f'frames/decision_boundary_{i}.png'\n",
    "    images.append(imageio.imread(filename))\n",
    "\n",
    "imageio.mimsave('decision_boundary_evolution1.gif', images, fps=5)  # Save GIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
