{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data = load_diabetes()\n",
    "X = diabetes_data.data\n",
    "y = diabetes_data.target\n",
    "num_samples = X.shape[0]  #number of samples\n",
    "num_features = X.shape[1]  #number of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(309, 10)\n",
      "(309, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.72744456, -0.90423839, ...,  0.80023299,\n",
       "         0.77090906,  1.3952692 ],\n",
       "       [ 1.        , -0.81900311,  1.10590306, ...,  1.60659854,\n",
       "         0.80834772,  0.38300972],\n",
       "       [ 1.        ,  0.90359385,  1.10590306, ..., -0.00613255,\n",
       "         0.69353582,  0.12994485],\n",
       "       ...,\n",
       "       [ 1.        , -0.44452551, -0.90423839, ..., -1.61886365,\n",
       "        -0.74545288, -0.88231462],\n",
       "       [ 1.        , -1.04368967,  1.10590306, ...,  2.41296409,\n",
       "         1.81611979,  0.8047845 ],\n",
       "       [ 1.        ,  0.45422073,  1.10590306, ...,  0.80023299,\n",
       "         1.0193482 ,  1.81704398]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "# print(X_train.shape)\n",
    "# actually you can do like this too\n",
    "# X = np.insert(X, 0, 1, axis=1)\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)\n",
    "# print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed form solution Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent gives one possible mean for minimizing $J$, which uses iterative approach and may take time.  In the situation where we know that our cost function is strictly concave or convex, we can explicitly take its derivative to zero.  This process of such derivation is called obtaining the **normal equations** or **closed form**. \n",
    "\n",
    "The **closed form** of linear regression can be derived easily.  Let $\\mathbf{X}$ be a matrix of shape $(m, n)$, $\\boldsymbol{\\theta}$ as shape $(n, )$, and $\\mathbf{y}$ as vector of shape $(m, )$.  Instead of writing the cost function as power of square, we shall write it in matrix multiplication as follows:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} (\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y})^T*(\\mathbf{X}\\boldsymbol{\\theta}-\\mathbf{y})$$\n",
    "\n",
    "Recall the following properties:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{X}} \\mathbf{X}^T\\mathbf{X}=2\\mathbf{X} \\tag{A}$$\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{X}} \\mathbf{A}\\mathbf{X}=\\mathbf{A}^T$$\n",
    "$$(\\mathbf{X}\\mathbf{y})^T = \\mathbf{y}^T\\mathbf{X}^T$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} (\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y})^T*(\\mathbf{X}\\boldsymbol{\\theta}-\\mathbf{y}) &= \\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} (\\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{y}^T\\mathbf{y})\\\\\n",
    "&= 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} - 2\\mathbf{X}^T\\mathbf{y} \\tag{see note*}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now, we can set the derivative to 0 to find out the optimal theta\n",
    "\n",
    "$$\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{X}^T\\mathbf{y} = 0$$\n",
    "\n",
    "Solving this gives us\n",
    "\n",
    "$$\\boldsymbol{\\theta} =  (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "\n",
    "Note*: Since $\\mathbf{X}\\boldsymbol{\\theta}$ is a vector, and so is $\\mathbf{y}$, it doesn't matter what the order is, thus we can simply add them to 2.  Also, we got 2 in front of the first part because we have two $\\theta$ (used the property A)\n",
    "\n",
    "\n",
    "**Why not closed form always**.  The answer is simple.  It does not always exists or possible, for example, the cost function is not convex or concave.  But of course, if it exists, we usually prefer closed form given that it is usually faster than gradient descent.  Nevertheless, as you can see, taking inverse of huge number of features can be expensive, thus it is also not always straightforward thing to always prefer closed form.\n",
    "\n",
    "Yes, that's it for most of the theoretical stuff.  Let's start implementing some of these concepts so we can better understand them.\n",
    "\n",
    "The closed form is a normal equations derived from setting the derivatives = 0.  By performing only some inverse operations and matrix multiplication, we will be able to get the theta.\n",
    "\n",
    "$$\\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "When closed form is available, is doable (can be inversed - can use pseudoinverse), and with not many features (i.e., inverse can be slow), it is recommended to always use closed form.  \n",
    "\n",
    "## Implementation steps:\n",
    "\n",
    "1. Prepare your data\n",
    "    - add intercept\n",
    "    - $\\mathbf{X}$ and $\\mathbf{y}$ and $\\mathbf{w}$ in the right shape\n",
    "        - $\\mathbf{X}$ -> $(m, n)$\n",
    "        - $\\mathbf{y}$ -> $(m, )$\n",
    "        - $\\mathbf{w}$ -> $(n, )$\n",
    "        - where $m$ is number of samples\n",
    "        - where $n$ is number of features\n",
    "    - train-test split\n",
    "    - feature scale\n",
    "    - clean out any missing data\n",
    "    - (optional) feature engineering\n",
    "2. Plug everything into the equation.  Here we shall use X_train to retrieve the $\\boldsymbol{\\theta}$\n",
    "$$\\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "1. We simply using the $\\boldsymbol{\\theta}$, we can perform a dot product with our X_test which will give us $\\mathbf{\\hat{y}}$.\n",
    "\n",
    "2. We then calculate the errors using mean-squared-error function:\n",
    "\n",
    "$$\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Note that it's a bit different from our $J(\\boldsymbol{\\theta})$ because $J(\\boldsymbol{\\theta})$ puts $\\frac{1}{2}$ instead of $\\frac{1}{m}$ for mathematical convenience for derivatives, since we know changing constants do not change the optimization results.\n",
    "\n",
    "\n",
    "Let's implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "class LinearRegression:\n",
    "    def __init__(self, num_features):\n",
    "        self.W = np.zeros(num_features)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.W = inv(X.T @ X) @ X.T @ y\n",
    "        return self.W\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = X @ self.W\n",
    "        return y_pred\n",
    "    \n",
    "    def eval(self, y_true, y_pred):\n",
    "        assert y_true.shape == y_pred.shape\n",
    "        mse = ((y_true - y_pred)**2).sum() / len(y_train)\n",
    "        print(\"Mean squared errors: \", mse)\n",
    "        return mse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared errors:  1290.8513558924972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1290.8513558924972"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression(num_features=num_features)\n",
    "W_star = model.fit(X=X_train, y=y_train)\n",
    "y_pred = model.predict(X=X_test)\n",
    "# print(y_pred.shape)\n",
    "# print(y_test.shape)\n",
    "model.eval(y_test,\n",
    "           y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
